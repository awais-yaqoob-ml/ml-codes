{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwm86JLIOtVF4fta4xCUl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awais-yaqoob-ml/ml-codes/blob/main/playwriterRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meRb9yjRYY8e",
        "outputId": "c8f0884d-55cc-4455-b007-b0bd45635549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "from keras.datasets import imdb\n",
        "import tensorflow as tf\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TOVAOEvZU2A",
        "outputId": "33d92043-b524-4431-a5cf-3360a9df38e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text charachters is {len(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF8l1jjMaaXt",
        "outputId": "1e2166bc-61d9-4f79-c0f5-35950c7dae92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text charachters is 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###encoding\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "vMnB5zw7a4e1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:18])\n",
        "print(text_to_int(text[:18]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot5isEGwcRQ0",
        "outputId": "f584435f-b3d3-40bd-a9c9-d5561cee5423"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Bef\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(integers):\n",
        "  try:\n",
        "    integers = integers.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[integers])\n",
        "\n",
        "print(int_to_text(text_to_int(text[:18])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFVPhXcfchUb",
        "outputId": "720aa08a-dd41-4b8c-8d08-92fb7bcce702"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Bef\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### make dataset\n",
        "seq_len = 100\n",
        "examples_per_epoch = len(text)//(seq_len+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "metadata": {
        "id": "woK018N-dO39"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "PmYbFIAgeF18"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "dJh8iBBpek5B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "CWj7_QRXfI4E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##build model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ierl6bRsgBP2",
        "outputId": "279d6268-4836-4201-f7a1-aca6d128e18b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "O9Hro1rnv6OO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "tqc5LUzYydX_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt_{epoch}')\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_prefix, save_weights_only=True)"
      ],
      "metadata": {
        "id": "V19do-xry5uT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsAMUrKY0Gv6",
        "outputId": "34def395-a749-43be-ecb4-b130d751b224"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 21s 70ms/step - loss: 2.5568\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.8718\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.6277\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.4953\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.4176\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.3617\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.3171\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.2789\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.2432\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.2084\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.1722\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.1361\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 1.1001\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.0607\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.0219\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.9810\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.9400\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.9004\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.8610\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.8235\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.7877\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 15s 71ms/step - loss: 0.7539\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.7241\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.6944\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.6672\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.6445\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.6230\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.6025\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.5854\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.5699\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.5540\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.5416\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.5296\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.5187\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.5095\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 15s 72ms/step - loss: 0.4999\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4911\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.4842\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4771\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, 1)"
      ],
      "metadata": {
        "id": "R66VimnV07ej"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "XVZghZgM3mM3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate=800\n",
        "  input_eval=[char2idx[s] for s in start_string]\n",
        "  input_eval=tf.expand_dims(input_eval,0)\n",
        "  text_generated = []\n",
        "  temperature=1.0\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions/temperature\n",
        "    predict_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval = tf.expand_dims([predict_id], 0)\n",
        "    text_generated.append(idx2char[predict_id])\n",
        "    # print(text_generated)\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "dHAczOLs59T5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input('input txt')\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIsHwRaO87C6",
        "outputId": "e3b92446-ff34-4ae1-d7fe-6e09f0a83fdc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input txtjoe\n",
            "joen, like thou\n",
            "Whose names?\n",
            "\n",
            "ROMEO:\n",
            "I stretch'd and follow them I am not too young Duke of York;\n",
            "Therefore Warwick, our barroughour night: to your sufficital\n",
            "Than is the pot he did before 't.\n",
            "\n",
            "POLIXENES:\n",
            "I'll draw the heir a fearful lad\n",
            "With tears and ears, and so it is in Padua;\n",
            "And what appearing in our cousins\n",
            "Exforce thy will be welk, who buddle joy\n",
            "Sell throne of mine, and then durantime sours themselves all the\n",
            "rebellion and defen them;\n",
            "Like over than by God's richmost of a widow's; new appracious in praments, but be gone.\n",
            "\n",
            "ROMEO:\n",
            "Heavens!\n",
            "What is't? What hast thou of golden stord\n",
            "The law upon a second her our reeches supposite\n",
            "Make pale at me, and my sweet son,\n",
            "Seeing thou hast provided me to save strong right,\n",
            "What our contrmem tyranny be required!\n",
            "What look in thy maid!\n",
            "\n",
            "PAULINA:\n",
            "I \n"
          ]
        }
      ]
    }
  ]
}